{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Codigo para geração do indice invertido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import nltk\n",
    "from nltk import bigrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import numpy as np\n",
    "import string\n",
    "from scipy import sparse\n",
    "import scipy.sparse as sps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class WordInfo:\n",
    "    'Classe que guarda informações sobre a palavra, como documentos em que ocorre, tf em cada documento e calculo de idf'\n",
    "\n",
    "    def __init__(self, word) : # Contrutor da classe recebe apenas a palavra em questão\n",
    "        self.word = word\n",
    "        self.idf = 0\n",
    "        self.docs = {} # Dicionario que mapeia doc_id ao tf da palavra\n",
    "\n",
    "    def found(self, doc_id) : # Metodo que realiza a contagem do tf da palavra\n",
    "        if(doc_id in self.docs) : # Se o doc_id está mapeado, incrementa-o\n",
    "            self.docs[doc_id] += 1\n",
    "        else :\n",
    "            self.docs[doc_id] = 1 # Caso contrario, define-o como 1\n",
    "            \n",
    "    def calculateIDF(self, totaldocs) : # Metodo de calculo do idf, recebendo o total de documentos\n",
    "        df = len(self.docs) # Numero de documentos em que a palavra ocorre \n",
    "        if (df > 0) :\n",
    "            self.idf = math.log(((totaldocs + 1)/df))\n",
    "            \n",
    "    def getIds(self) : # Metodo que retorna todos os doc_ids em que a palavra ocorre\n",
    "        return list(self.docs.keys())\n",
    "    \n",
    "    def getTf(self, doc_id) : # Metodo que retorna o tf da palavra em um documento, ou 0 caso não ocorra\n",
    "        return self.docs.get(doc_id, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tabela = pd.read_csv(\"estadao_noticias_eleicao.csv\", encoding=\"utf-8\")\n",
    "tabela.fillna('', inplace=True) # Preenchendo os campos vazios da tabela com ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "content = tabela.titulo + \" \" + tabela.subTitulo + \" \" +  tabela.conteudo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokens_lists = content.apply(lambda text: tokenizer.tokenize(text.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = [token for tokens_list in tokens_lists for token in tokens_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = set(corpus)\n",
    "vocab = list(vocab)\n",
    "vocab = {word:i for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mapa = {} # Dicionario/Mapa que representa o indice invertido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documentos = {} # Dicionario que guarda os vetores dos documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, linha in tabela.iterrows() : # Iterando sobre as noticias no arquivo\n",
    "    id = linha['idNoticia'] # Recuperando o idNoticia da noticia atual\n",
    "    texto = tokenizer.tokenize(linha['titulo'] + ' ' + linha['subTitulo'] + ' ' + linha['conteudo']) # Tokenização do texto\n",
    "    documentos[id] = {}\n",
    "    for palavra in texto: # Iterando sobre as palavras na Noticia\n",
    "        if (palavra not in string.punctuation): # Só ocorre o mapeamento se a palavra não é uma pontuação\n",
    "            if (palavra.lower() not in mapa) : # Se a palavra ainda não foi mapeada, mapeia-a\n",
    "                mapa[palavra.lower()] = WordInfo(palavra.lower())\n",
    "            mapa[palavra.lower()].found(id) # Contabiliza a ocorrencia da palavra no documento atual\n",
    "            documentos[id][vocab[palavra.lower()]] = 1\n",
    "\n",
    "for k in mapa.keys() : # Laço para realizar o calculo dos idfs de cada palavra mapeada\n",
    "    mapa[k].calculateIDF(len(documentos))\n",
    "    index_palavra = vocab[k]\n",
    "    for doc_i in documentos.keys() :\n",
    "        if (index_palavra in documentos[doc_i]) :\n",
    "            documentos[doc_i][index_palavra] = mapa[k].getTf(doc_i) * mapa[k].idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funções de Consultas Booleanas(And, Or e geral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def searchAnd(palavra1, *palavras) :\n",
    "    docs1 = mapa[palavra1.lower()].getIds() # Recupera todos as noticias em que \"palavra1\" ocorreu\n",
    "    docs2 = mapa[palavra2.lower()].getIds() # Recupera todos as noticias em que \"palavra2\" ocorreu\n",
    "    result = set() # Cria um conjunto vazio\n",
    "    result.update(docs1) # Preenche o conjunto com os ids das noticias que \"palavra1\" aparece\n",
    "    for palavra in palavras :\n",
    "        docs = mapa[palavra.lower()].getIds()\n",
    "        result = result.intersection(docs)\n",
    "    return list(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def searchOr(palavra1, *palavras) :\n",
    "    docs1 = mapa[palavra1.lower()].getIds()\n",
    "    result = set() # Cria um conjunto vazio\n",
    "    result.update(docs1) # Preenche o conjunto com os ids das noticias que \"palavra1\" aparece\n",
    "    for palavra in palavras :\n",
    "        docs = mapa[palavra.lower()].getIds()\n",
    "        result.update(docs)\n",
    "    return list(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search(consulta) :\n",
    "    partes = consulta.split(' ')\n",
    "    if (len(partes) < 2) : # Se a consulta só tem uma palavra\n",
    "        return mapa[partes[0].lower()].getIds() # Recupera os ids que a palavra aparece\n",
    "    elif (partes[1].upper() == 'AND') : # Se é uma consulta AND\n",
    "        return searchAnd(partes[0], partes[2]) # Chama a função de consulta AND\n",
    "    elif (partes[1].upper() == 'OR') : # Se é uma consulta OR\n",
    "        return searchOr(partes[0], partes[2]) # Chama a função de consulta OR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funções de Consultas Vetoriais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def busca_binaria(consulta) :\n",
    "    palavras = consulta.split(' ') # Quebrando a consulta em palavras\n",
    "    relevant_docs = set(mapa[palavras[0]].getIds()) # Inicia o conjunto de documentos relevantes\n",
    "    # Laço que realiza interseção dos conjuntos, para mantes apenas documentos em que todas as palavras a consulta ocorre\n",
    "    for i in range(1, len(palavras)) :\n",
    "        relevant_docs = relevant_docs.intersection(set(mapa[palavras[i]].getIds()))\n",
    "    # Como é busca binaria, apenas retorna os primeiros 5 documentos que contem todas as palavras\n",
    "    return list(relevant_docs)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def busca_tf(consulta) :\n",
    "    palavras = consulta.split(' ') # Quebrando a consulta em palavras\n",
    "    relevant_docs = set(mapa[palavras[0]].getIds()) # Inicia o conjunto de documentos relevantes\n",
    "    # Laço que realiza interseção dos conjuntos, para mantes apenas documentos em que todas as palavras a consulta ocorre\n",
    "    for i in range(1, len(palavras)) :\n",
    "        relevant_docs = relevant_docs.intersection(set(mapa[palavras[i]].getIds()))\n",
    "    result = [] # Lista que será gerado o resultado\n",
    "    for doc_id in relevant_docs : # Para cada documento relevante\n",
    "        scores = [mapa[w].getTf(doc_id) for w in palavras] # Obtem os tfs de cada palavra da consulta\n",
    "        # Cria uma tupla (score, doc_id) somando os tfs obtidos e coloca na lista\n",
    "        score_id = (sum(scores), doc_id)\n",
    "        result.append(score_id)\n",
    "    # Ordena do maior para o menor, considerando apenas o primeiro elemento da tupla para ordenar (o score)\n",
    "    result = sorted(result, reverse=True, key=lambda tup: tup[0]) \n",
    "    result = [t[1] for t in result] # A lista passa a ser apenas dos doc_ids\n",
    "    return result[:5] # Retorna os 5 primeiros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def busca_tfidf(consulta) :\n",
    "    palavras = consulta.split(' ') # Quebrando a consulta em palavras\n",
    "    relevant_docs = set(mapa[palavras[0]].getIds()) # Inicia o conjunto de documentos relevantes\n",
    "    # Laço que realiza interseção dos conjuntos, para mantes apenas documentos em que todas as palavras a consulta ocorre\n",
    "    for i in range(1, len(palavras)) :\n",
    "        relevant_docs = relevant_docs.intersection(set(mapa[palavras[i]].getIds()))\n",
    "    result = [] # Lista que será gerado o resultado\n",
    "    for doc_id in relevant_docs : # Para cada documento relevante\n",
    "        scores = [] # Lista para guardar os scores de cada palavra da consulta\n",
    "        for w in palavras : # Calcula tf * idf de cada palavra da consulta e adiciona na lista\n",
    "            m_palavra = mapa[w]\n",
    "            w_score = m_palavra.getTf(doc_id) * m_palavra.idf\n",
    "            scores.append(w_score)\n",
    "        # Cria uma tupla (score, doc_id) somando os resultados obtidos e coloca na lista\n",
    "        score_id = (sum(scores), doc_id)\n",
    "        result.append(score_id)\n",
    "    # Ordena do maior para o menor, considerando apenas o primeiro elemento da tupla para ordenar (o score)\n",
    "    result = sorted(result, reverse=True, key=lambda tup: tup[0])\n",
    "    result = [t[1] for t in result] # A lista passa a ser apenas dos doc_ids\n",
    "    return result[:5] # Retorna os 5 primeiros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def busca_bm25(consulta) :\n",
    "    k = 6 # 6 foi o valor de K que maximizou o resultado da função mapk\n",
    "    palavras = consulta.split(' ') # Quebrando a consulta em palavras\n",
    "    relevant_docs = set(mapa[palavras[0]].getIds()) # Inicia o conjunto de documentos relevantes\n",
    "    # Laço que realiza interseção dos conjuntos, para mantes apenas documentos em que todas as palavras a consulta ocorre\n",
    "    for i in range(1, len(palavras)) :\n",
    "        relevant_docs = relevant_docs.intersection(set(mapa[palavras[i]].getIds()))\n",
    "    result = [] # Lista que será gerado o resultado\n",
    "    for doc_id in relevant_docs : # Para cada documento relevante\n",
    "        scores = [] # Lista para guardar os scores de cada palavra da consulta\n",
    "        for w in palavras : # Calcula tf*(k+1)/(tf + k) * idf de cada palavra da consulta e adiciona na lista\n",
    "            m_palavra = mapa[w]\n",
    "            tf = m_palavra.getTf(doc_id)\n",
    "            w_score = (tf*(k+1))/(tf + k) * m_palavra.idf\n",
    "            scores.append(w_score)\n",
    "        # Cria uma tupla (score, doc_id) somando os resultados obtidos e coloca na lista\n",
    "        score_id = (sum(scores), doc_id)\n",
    "        result.append(score_id)\n",
    "    # Ordena do maior para o menor, considerando apenas o primeiro elemento da tupla para ordenar (o score)\n",
    "    result = sorted(result, reverse=True, key=lambda tup: tup[0])\n",
    "    result = [t[1] for t in result] # A lista passa a ser apenas dos doc_ids\n",
    "    return result[:5] # Retorna os 5 primeiros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funções para calcular a similaridade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def distance(docQ, docI) :\n",
    "    score = 0\n",
    "    for word_index in docQ.keys() :\n",
    "        score += docQ[word_index] * docI.get(word_index, 0)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def top5(docQ) :\n",
    "    top = [(id_doc, distance(docQ, documentos[id_doc])) for id_doc in list(documentos.keys())[:5]]\n",
    "    top = sorted(top, reverse=True, key=lambda tup: tup[1])\n",
    "    for i in list(documentos.keys())[5:]:\n",
    "        d = distance(docQ, documentos[i])\n",
    "        j = 4\n",
    "        while (d > top[j][1] and j > 0) :\n",
    "            j -= 1\n",
    "        top.insert(j+1, (i, d))\n",
    "        top = top[:5]\n",
    "    return top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 10373.831340546792),\n",
       " (7, 4539.375061954259),\n",
       " (6554, 4124.683707865925),\n",
       " (3942, 3893.220158262253),\n",
       " (7017, 3387.111891626281)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top5(documentos[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
