{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Codigo para geração do indice invertido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import nltk\n",
    "from nltk import bigrams \n",
    "import numpy as np\n",
    "import string\n",
    "from scipy import sparse\n",
    "import scipy.sparse as sps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class WordInfo:\n",
    "    'Classe que guarda informações sobre a palavra, como documentos em que ocorre, tf em cada documento e calculo de idf'\n",
    "\n",
    "    def __init__(self, word) : # Contrutor da classe recebe apenas a palavra em questão\n",
    "        self.word = word\n",
    "        self.idf = 0\n",
    "        self.docs = {} # Dicionario que mapeia doc_id ao tf da palavra\n",
    "\n",
    "    def found(self, doc_id) : # Metodo que realiza a contagem do tf da palavra\n",
    "        if(doc_id in self.docs) : # Se o doc_id está mapeado, incrementa-o\n",
    "            self.docs[doc_id] += 1\n",
    "        else :\n",
    "            self.docs[doc_id] = 1 # Caso contrario, define-o como 1\n",
    "            \n",
    "    def calculateIDF(self, totaldocs) : # Metodo de calculo do idf, recebendo o total de documentos\n",
    "        df = len(self.docs) # Numero de documentos em que a palavra ocorre \n",
    "        if (df > 0) :\n",
    "            self.idf = math.log(((totaldocs + 1)/df))\n",
    "            \n",
    "    def getIds(self) : # Metodo que retorna todos os doc_ids em que a palavra ocorre\n",
    "        return list(self.docs.keys())\n",
    "    \n",
    "    def getTf(self, doc_id) : # Metodo que retorna o tf da palavra em um documento, ou 0 caso não ocorra\n",
    "        return self.docs.get(doc_id, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tabela = pd.read_csv(\"estadao_noticias_eleicao.csv\", encoding=\"utf-8\")\n",
    "tabela.fillna('', inplace=True) # Preenchendo os campos vazios da tabela com ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mapa = {} # Dicionario/Mapa que representa o indice invertido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documentos = {} # Dicionario que guarda os documentos e seus tamanhos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, linha in tabela.iterrows() : # Iterando sobre as noticias no arquivo\n",
    "    id = linha['idNoticia'] # Recuperando o idNoticia da noticia atual\n",
    "    texto = nltk.wordpunct_tokenize(linha['titulo'] + ' ' + linha['subTitulo'] + ' ' + linha['conteudo']) # Tokenização do texto\n",
    "    documentos[id] = len(texto) # Mapeamento do documento com seu tamanho\n",
    "    for palavra in texto: # Iterando sobre as palavras na Noticia\n",
    "        if (palavra not in string.punctuation): # Só ocorre o mapeamento se a palavra não é uma pontuação\n",
    "            if (palavra.lower() not in mapa) : # Se a palavra ainda não foi mapeada, mapeia-a\n",
    "                mapa[palavra.lower()] = WordInfo(palavra.lower())\n",
    "            mapa[palavra.lower()].found(id) # Contabiliza a ocorrencia da palavra no documento atual\n",
    "\n",
    "for k in mapa.keys() : # Laço para realizar o calculo dos idfs de cada palavra mapeada\n",
    "    mapa[k].calculateIDF(len(documentos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funções de Consultas Booleanas(And, Or e geral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def searchAnd(palavra1, palavra2) :\n",
    "    docs1 = mapa[palavra1.lower()].getIds() # Recupera todos as noticias em que \"palavra1\" ocorreu\n",
    "    docs2 = mapa[palavra2.lower()].getIds() # Recupera todos as noticias em que \"palavra2\" ocorreu\n",
    "    result = set() # Cria um conjunto vazio\n",
    "    result.update(docs1) # Preenche o conjunto com os ids das noticias que \"palavra1\" aparece\n",
    "    result = result.intersection(docs2) # Mantem no conjunto apenas os ids que as duas palavras ocorrem.\n",
    "    return list(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def searchOr(palavra1, palavra2) :\n",
    "    docs1 = mapa[palavra1.lower()].getIds() # Recupera todos as noticias em que \"palavra1\" ocorreu\n",
    "    docs2 = mapa[palavra2.lower()].getIds() # Recupera todos as noticias em que \"palavra2\" ocorreu\n",
    "    result = set() # Cria um conjunto vazio\n",
    "    result.update(docs1) # Preenche o conjunto com os ids das noticias que \"palavra1\" aparece\n",
    "    result.update(docs2) # Preenche o conjunto com os ids das noticias que \"palavra2\" aparece, por ser conjunto as duplicadas são eliminadas\n",
    "    return list(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search(consulta) :\n",
    "    partes = consulta.split(' ')\n",
    "    if (len(partes) < 2) : # Se a consulta só tem uma palavra\n",
    "        return mapa[partes[0].lower()].getIds() # Recupera os ids que a palavra aparece\n",
    "    elif (partes[1].upper() == 'AND') : # Se é uma consulta AND\n",
    "        return searchAnd(partes[0], partes[2]) # Chama a função de consulta AND\n",
    "    elif (partes[1].upper() == 'OR') : # Se é uma consulta OR\n",
    "        return searchOr(partes[0], partes[2]) # Chama a função de consulta OR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funções de Consultas Vetoriais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def busca_binaria(consulta) :\n",
    "    palavras = consulta.split(' ') # Quebrando a consulta em palavras\n",
    "    relevant_docs = set(mapa[palavras[0]].getIds()) # Inicia o conjunto de documentos relevantes\n",
    "    # Laço que realiza interseção dos conjuntos, para mantes apenas documentos em que todas as palavras a consulta ocorre\n",
    "    for i in range(1, len(palavras)) :\n",
    "        relevant_docs = relevant_docs.intersection(set(mapa[palavras[i]].getIds()))\n",
    "    # Como é busca binaria, apenas retorna os primeiros 5 documentos que contem todas as palavras\n",
    "    return list(relevant_docs)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def busca_tf(consulta) :\n",
    "    palavras = consulta.split(' ') # Quebrando a consulta em palavras\n",
    "    relevant_docs = set(mapa[palavras[0]].getIds()) # Inicia o conjunto de documentos relevantes\n",
    "    # Laço que realiza interseção dos conjuntos, para mantes apenas documentos em que todas as palavras a consulta ocorre\n",
    "    for i in range(1, len(palavras)) :\n",
    "        relevant_docs = relevant_docs.intersection(set(mapa[palavras[i]].getIds()))\n",
    "    result = [] # Lista que será gerado o resultado\n",
    "    for doc_id in relevant_docs : # Para cada documento relevante\n",
    "        scores = [mapa[w].getTf(doc_id) for w in palavras] # Obtem os tfs de cada palavra da consulta\n",
    "        # Cria uma tupla (score, doc_id) somando os tfs obtidos e coloca na lista\n",
    "        score_id = (sum(scores), doc_id)\n",
    "        result.append(score_id)\n",
    "    # Ordena do maior para o menor, considerando apenas o primeiro elemento da tupla para ordenar (o score)\n",
    "    result = sorted(result, reverse=True, key=lambda tup: tup[0]) \n",
    "    result = [t[1] for t in result] # A lista passa a ser apenas dos doc_ids\n",
    "    return result[:5] # Retorna os 5 primeiros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def busca_tfidf(consulta) :\n",
    "    palavras = consulta.split(' ') # Quebrando a consulta em palavras\n",
    "    relevant_docs = set(mapa[palavras[0]].getIds()) # Inicia o conjunto de documentos relevantes\n",
    "    # Laço que realiza interseção dos conjuntos, para mantes apenas documentos em que todas as palavras a consulta ocorre\n",
    "    for i in range(1, len(palavras)) :\n",
    "        relevant_docs = relevant_docs.intersection(set(mapa[palavras[i]].getIds()))\n",
    "    result = [] # Lista que será gerado o resultado\n",
    "    for doc_id in relevant_docs : # Para cada documento relevante\n",
    "        scores = [] # Lista para guardar os scores de cada palavra da consulta\n",
    "        for w in palavras : # Calcula tf * idf de cada palavra da consulta e adiciona na lista\n",
    "            m_palavra = mapa[w]\n",
    "            w_score = m_palavra.getTf(doc_id) * m_palavra.idf\n",
    "            scores.append(w_score)\n",
    "        # Cria uma tupla (score, doc_id) somando os resultados obtidos e coloca na lista\n",
    "        score_id = (sum(scores), doc_id)\n",
    "        result.append(score_id)\n",
    "    # Ordena do maior para o menor, considerando apenas o primeiro elemento da tupla para ordenar (o score)\n",
    "    result = sorted(result, reverse=True, key=lambda tup: tup[0])\n",
    "    result = [t[1] for t in result] # A lista passa a ser apenas dos doc_ids\n",
    "    return result[:5] # Retorna os 5 primeiros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def busca_bm25(consulta) :\n",
    "    k = 6 # 6 foi o valor de K que maximizou o resultado da função mapk\n",
    "    palavras = consulta.split(' ') # Quebrando a consulta em palavras\n",
    "    relevant_docs = set(mapa[palavras[0]].getIds()) # Inicia o conjunto de documentos relevantes\n",
    "    # Laço que realiza interseção dos conjuntos, para mantes apenas documentos em que todas as palavras a consulta ocorre\n",
    "    for i in range(1, len(palavras)) :\n",
    "        relevant_docs = relevant_docs.intersection(set(mapa[palavras[i]].getIds()))\n",
    "    result = [] # Lista que será gerado o resultado\n",
    "    for doc_id in relevant_docs : # Para cada documento relevante\n",
    "        scores = [] # Lista para guardar os scores de cada palavra da consulta\n",
    "        for w in palavras : # Calcula tf*(k+1)/(tf + k) * idf de cada palavra da consulta e adiciona na lista\n",
    "            m_palavra = mapa[w]\n",
    "            tf = m_palavra.getTf(doc_id)\n",
    "            w_score = (tf*(k+1))/(tf + k) * m_palavra.idf\n",
    "            scores.append(w_score)\n",
    "        # Cria uma tupla (score, doc_id) somando os resultados obtidos e coloca na lista\n",
    "        score_id = (sum(scores), doc_id)\n",
    "        result.append(score_id)\n",
    "    # Ordena do maior para o menor, considerando apenas o primeiro elemento da tupla para ordenar (o score)\n",
    "    result = sorted(result, reverse=True, key=lambda tup: tup[0])\n",
    "    result = [t[1] for t in result] # A lista passa a ser apenas dos doc_ids\n",
    "    return result[:5] # Retorna os 5 primeiros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avaliação Buscas Vetoriais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definição da função de comparação, Leitura do gabarito e realização das consultas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apk(actual, predicted, k=10):\n",
    "    if len(predicted)>k:\n",
    "        predicted = predicted[:k]\n",
    "\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "\n",
    "    for i,p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i+1.0)\n",
    "\n",
    "    if not actual:\n",
    "        return 0.0\n",
    "\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "def mapk(actual, predicted, k=10):\n",
    "    return np.mean([apk(a,p,k) for a,p in zip(actual, predicted)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "gabarito = pandas.read_csv('gabarito.csv')\n",
    "# Transformando a string lida em lista de listas\n",
    "gabarito_busca_binaria = list(gabarito.busca_binaria)\n",
    "gabarito_tf = list(gabarito.tf)\n",
    "gabarito_tfidf = list(gabarito.tfidf)\n",
    "gabarito_bm25 = list(gabarito.bm25)\n",
    "for i in range(len(gabarito_tf)) :\n",
    "    gabarito_busca_binaria[i] = ast.literal_eval(gabarito_busca_binaria[i])\n",
    "    gabarito_tf[i] = ast.literal_eval(gabarito_tf[i])\n",
    "    gabarito_tfidf[i] = ast.literal_eval(gabarito_tfidf[i])\n",
    "    gabarito_bm25[i] = ast.literal_eval(gabarito_bm25[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "binaria_result, tf_result, tfidf_result, bm25_result = [], [], [], []\n",
    "for consulta in gabarito.str_busca :\n",
    "    binaria_result.append(busca_binaria(consulta))\n",
    "    tf_result.append(busca_tf(consulta))\n",
    "    tfidf_result.append(busca_tfidf(consulta))\n",
    "    bm25_result.append(busca_bm25(consulta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expansão de Consulta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Função de criação da matrix de termos-termos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def co_occurrence_matrix(corpus):\n",
    "    vocab = set(corpus)\n",
    "    vocab = list(vocab)\n",
    "    n = len(vocab)\n",
    "   \n",
    "    vocab_to_index = {word:i for i, word in enumerate(vocab)}\n",
    "    \n",
    "    bi_grams = list(bigrams(corpus))\n",
    "\n",
    "    bigram_freq = nltk.FreqDist(bi_grams).most_common(len(bi_grams))\n",
    "\n",
    "    I=list()\n",
    "    J=list()\n",
    "    V=list()\n",
    "    \n",
    "    for bigram in bigram_freq:\n",
    "        current = bigram[0][1]\n",
    "        previous = bigram[0][0]\n",
    "        count = bigram[1]\n",
    "\n",
    "        I.append(vocab_to_index[previous])\n",
    "        J.append(vocab_to_index[current])\n",
    "        V.append(count)\n",
    "        \n",
    "    co_occurrence_matrix = sparse.coo_matrix((V,(I,J)), shape=(n,n))\n",
    "\n",
    "    return co_occurrence_matrix, vocab_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = tabela.titulo + \" \" + tabela.subTitulo + \" \" +  tabela.conteudo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_lists = content.apply(lambda text: nltk.wordpunct_tokenize(text.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [token for tokens_list in tokens_lists for token in tokens_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix, vocab = co_occurrence_matrix(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "consultable_matrix = matrix.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consult_frequency(w1, w2):\n",
    "    return(consultable_matrix[vocab[w1],vocab[w2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top3_bigram(palavra) :\n",
    "    bigram_freq = []\n",
    "    for key in vocab.keys() :\n",
    "        bigram_freq.append((consult_frequency(palavra, key), key))\n",
    "    bigram_freq = sorted(bigram_freq, reverse=True)[:3]\n",
    "    return [t[1] for t in bigram_freq]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
