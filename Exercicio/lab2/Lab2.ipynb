{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Codigo para geração do indice invertido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import nltk\n",
    "from nltk import bigrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import numpy as np\n",
    "import string\n",
    "from scipy import sparse\n",
    "import scipy.sparse as sps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordInfo:\n",
    "    'Classe que guarda informações sobre a palavra, como documentos em que ocorre, tf em cada documento e calculo de idf'\n",
    "\n",
    "    def __init__(self, word) : # Contrutor da classe recebe apenas a palavra em questão\n",
    "        self.word = word\n",
    "        self.idf = 0\n",
    "        self.docs = {} # Dicionario que mapeia doc_id ao tf da palavra\n",
    "\n",
    "    def found(self, doc_id) : # Metodo que realiza a contagem do tf da palavra\n",
    "        if(doc_id in self.docs) : # Se o doc_id está mapeado, incrementa-o\n",
    "            self.docs[doc_id] += 1\n",
    "        else :\n",
    "            self.docs[doc_id] = 1 # Caso contrario, define-o como 1\n",
    "            \n",
    "    def calculateIDF(self, totaldocs) : # Metodo de calculo do idf, recebendo o total de documentos\n",
    "        df = len(self.docs) # Numero de documentos em que a palavra ocorre \n",
    "        if (df > 0) :\n",
    "            self.idf = math.log(((totaldocs + 1)/df))\n",
    "            \n",
    "    def getIds(self) : # Metodo que retorna todos os doc_ids em que a palavra ocorre\n",
    "        return list(self.docs.keys())\n",
    "    \n",
    "    def getTf(self, doc_id) : # Metodo que retorna o tf da palavra em um documento, ou 0 caso não ocorra\n",
    "        return self.docs.get(doc_id, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabela = pd.read_csv(\"estadao_noticias_eleicao.csv\", encoding=\"utf-8\")\n",
    "tabela.fillna('', inplace=True) # Preenchendo os campos vazios da tabela com ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapa = {} # Dicionario/Mapa que representa o indice invertido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentos = {} # Dicionario que guarda os documentos e seus tamanhos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, linha in tabela.iterrows() : # Iterando sobre as noticias no arquivo\n",
    "    id = linha['idNoticia'] # Recuperando o idNoticia da noticia atual\n",
    "    texto = nltk.wordpunct_tokenize(linha['titulo'] + ' ' + linha['subTitulo'] + ' ' + linha['conteudo']) # Tokenização do texto\n",
    "    documentos[id] = len(texto) # Mapeamento do documento com seu tamanho\n",
    "    for palavra in texto: # Iterando sobre as palavras na Noticia\n",
    "        if (palavra not in string.punctuation): # Só ocorre o mapeamento se a palavra não é uma pontuação\n",
    "            if (palavra.lower() not in mapa) : # Se a palavra ainda não foi mapeada, mapeia-a\n",
    "                mapa[palavra.lower()] = WordInfo(palavra.lower())\n",
    "            mapa[palavra.lower()].found(id) # Contabiliza a ocorrencia da palavra no documento atual\n",
    "\n",
    "for k in mapa.keys() : # Laço para realizar o calculo dos idfs de cada palavra mapeada\n",
    "    mapa[k].calculateIDF(len(documentos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funções de Consultas Booleanas(And, Or e geral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchAnd(palavra1, *palavras) :\n",
    "    docs1 = mapa[palavra1.lower()].getIds() # Recupera todos as noticias em que \"palavra1\" ocorreu\n",
    "    docs2 = mapa[palavra2.lower()].getIds() # Recupera todos as noticias em que \"palavra2\" ocorreu\n",
    "    result = set() # Cria um conjunto vazio\n",
    "    result.update(docs1) # Preenche o conjunto com os ids das noticias que \"palavra1\" aparece\n",
    "    for palavra in palavras :\n",
    "        docs = mapa[palavra.lower()].getIds()\n",
    "        result = result.intersection(docs)\n",
    "    return list(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchOr(palavra1, *palavras) :\n",
    "    docs1 = mapa[palavra1.lower()].getIds()\n",
    "    result = set() # Cria um conjunto vazio\n",
    "    result.update(docs1) # Preenche o conjunto com os ids das noticias que \"palavra1\" aparece\n",
    "    for palavra in palavras :\n",
    "        docs = mapa[palavra.lower()].getIds()\n",
    "        result.update(docs)\n",
    "    return list(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(consulta) :\n",
    "    partes = consulta.split(' ')\n",
    "    if (len(partes) < 2) : # Se a consulta só tem uma palavra\n",
    "        return mapa[partes[0].lower()].getIds() # Recupera os ids que a palavra aparece\n",
    "    elif (partes[1].upper() == 'AND') : # Se é uma consulta AND\n",
    "        return searchAnd(partes[0], partes[2]) # Chama a função de consulta AND\n",
    "    elif (partes[1].upper() == 'OR') : # Se é uma consulta OR\n",
    "        return searchOr(partes[0], partes[2]) # Chama a função de consulta OR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funções de Consultas Vetoriais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def busca_binaria(consulta) :\n",
    "    palavras = consulta.split(' ') # Quebrando a consulta em palavras\n",
    "    relevant_docs = set(mapa[palavras[0]].getIds()) # Inicia o conjunto de documentos relevantes\n",
    "    # Laço que realiza interseção dos conjuntos, para mantes apenas documentos em que todas as palavras a consulta ocorre\n",
    "    for i in range(1, len(palavras)) :\n",
    "        relevant_docs = relevant_docs.intersection(set(mapa[palavras[i]].getIds()))\n",
    "    # Como é busca binaria, apenas retorna os primeiros 5 documentos que contem todas as palavras\n",
    "    return list(relevant_docs)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def busca_tf(consulta) :\n",
    "    palavras = consulta.split(' ') # Quebrando a consulta em palavras\n",
    "    relevant_docs = set(mapa[palavras[0]].getIds()) # Inicia o conjunto de documentos relevantes\n",
    "    # Laço que realiza interseção dos conjuntos, para mantes apenas documentos em que todas as palavras a consulta ocorre\n",
    "    for i in range(1, len(palavras)) :\n",
    "        relevant_docs = relevant_docs.intersection(set(mapa[palavras[i]].getIds()))\n",
    "    result = [] # Lista que será gerado o resultado\n",
    "    for doc_id in relevant_docs : # Para cada documento relevante\n",
    "        scores = [mapa[w].getTf(doc_id) for w in palavras] # Obtem os tfs de cada palavra da consulta\n",
    "        # Cria uma tupla (score, doc_id) somando os tfs obtidos e coloca na lista\n",
    "        score_id = (sum(scores), doc_id)\n",
    "        result.append(score_id)\n",
    "    # Ordena do maior para o menor, considerando apenas o primeiro elemento da tupla para ordenar (o score)\n",
    "    result = sorted(result, reverse=True, key=lambda tup: tup[0]) \n",
    "    result = [t[1] for t in result] # A lista passa a ser apenas dos doc_ids\n",
    "    return result[:5] # Retorna os 5 primeiros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def busca_tfidf(consulta) :\n",
    "    palavras = consulta.split(' ') # Quebrando a consulta em palavras\n",
    "    relevant_docs = set(mapa[palavras[0]].getIds()) # Inicia o conjunto de documentos relevantes\n",
    "    # Laço que realiza interseção dos conjuntos, para mantes apenas documentos em que todas as palavras a consulta ocorre\n",
    "    for i in range(1, len(palavras)) :\n",
    "        relevant_docs = relevant_docs.intersection(set(mapa[palavras[i]].getIds()))\n",
    "    result = [] # Lista que será gerado o resultado\n",
    "    for doc_id in relevant_docs : # Para cada documento relevante\n",
    "        scores = [] # Lista para guardar os scores de cada palavra da consulta\n",
    "        for w in palavras : # Calcula tf * idf de cada palavra da consulta e adiciona na lista\n",
    "            m_palavra = mapa[w]\n",
    "            w_score = m_palavra.getTf(doc_id) * m_palavra.idf\n",
    "            scores.append(w_score)\n",
    "        # Cria uma tupla (score, doc_id) somando os resultados obtidos e coloca na lista\n",
    "        score_id = (sum(scores), doc_id)\n",
    "        result.append(score_id)\n",
    "    # Ordena do maior para o menor, considerando apenas o primeiro elemento da tupla para ordenar (o score)\n",
    "    result = sorted(result, reverse=True, key=lambda tup: tup[0])\n",
    "    result = [t[1] for t in result] # A lista passa a ser apenas dos doc_ids\n",
    "    return result[:5] # Retorna os 5 primeiros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def busca_bm25(consulta) :\n",
    "    k = 6 # 6 foi o valor de K que maximizou o resultado da função mapk\n",
    "    palavras = consulta.split(' ') # Quebrando a consulta em palavras\n",
    "    relevant_docs = set(mapa[palavras[0]].getIds()) # Inicia o conjunto de documentos relevantes\n",
    "    # Laço que realiza interseção dos conjuntos, para mantes apenas documentos em que todas as palavras a consulta ocorre\n",
    "    for i in range(1, len(palavras)) :\n",
    "        relevant_docs = relevant_docs.intersection(set(mapa[palavras[i]].getIds()))\n",
    "    result = [] # Lista que será gerado o resultado\n",
    "    for doc_id in relevant_docs : # Para cada documento relevante\n",
    "        scores = [] # Lista para guardar os scores de cada palavra da consulta\n",
    "        for w in palavras : # Calcula tf*(k+1)/(tf + k) * idf de cada palavra da consulta e adiciona na lista\n",
    "            m_palavra = mapa[w]\n",
    "            tf = m_palavra.getTf(doc_id)\n",
    "            w_score = (tf*(k+1))/(tf + k) * m_palavra.idf\n",
    "            scores.append(w_score)\n",
    "        # Cria uma tupla (score, doc_id) somando os resultados obtidos e coloca na lista\n",
    "        score_id = (sum(scores), doc_id)\n",
    "        result.append(score_id)\n",
    "    # Ordena do maior para o menor, considerando apenas o primeiro elemento da tupla para ordenar (o score)\n",
    "    result = sorted(result, reverse=True, key=lambda tup: tup[0])\n",
    "    result = [t[1] for t in result] # A lista passa a ser apenas dos doc_ids\n",
    "    return result[:5] # Retorna os 5 primeiros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expansão de Consulta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Função de criação da matrix de termos-termos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def co_occurrence_matrix(corpus):\n",
    "    vocab = set(corpus)\n",
    "    vocab = list(vocab)\n",
    "    n = len(vocab)\n",
    "   \n",
    "    vocab_to_index = {word:i for i, word in enumerate(vocab)}\n",
    "    \n",
    "    bi_grams = list(bigrams(corpus))\n",
    "\n",
    "    bigram_freq = nltk.FreqDist(bi_grams).most_common(len(bi_grams))\n",
    "\n",
    "    I=list()\n",
    "    J=list()\n",
    "    V=list()\n",
    "    \n",
    "    for bigram in bigram_freq:\n",
    "        current = bigram[0][1]\n",
    "        previous = bigram[0][0]\n",
    "        count = bigram[1]\n",
    "\n",
    "        I.append(vocab_to_index[previous])\n",
    "        J.append(vocab_to_index[current])\n",
    "        V.append(count)\n",
    "        \n",
    "    co_occurrence_matrix = sparse.coo_matrix((V,(I,J)), shape=(n,n))\n",
    "\n",
    "    return co_occurrence_matrix, vocab_to_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Montagem da matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = tabela.titulo + \" \" + tabela.subTitulo + \" \" +  tabela.conteudo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remoção de pontuação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokens_lists = content.apply(lambda text: tokenizer.tokenize(text.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remoção de stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\SauloSamuel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_ = stopwords.words('portuguese')\n",
    "filtered_tokens = tokens_lists.apply(lambda tokens: [token for token in tokens if token not in stopword_])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformando lista de listas em uma lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [token for tokens_list in filtered_tokens for token in tokens_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix, vocab = co_occurrence_matrix(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consulta de frequência de bigramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "consultable_matrix = matrix.tocsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Função de consulta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consult_frequency(w1, w2):\n",
    "    return(consultable_matrix[vocab[w1],vocab[w2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expandindo a consulta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Função que conta a coocorrência de uma palavra passada com todas as outras palavras do corpud e recupera as 3 que mais ocorrem com a palavra passada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top3_bigram(palavra) :\n",
    "    bigram_freq = []\n",
    "    for key in vocab.keys() :\n",
    "        bigram_freq.append((consult_frequency(palavra, key), key))\n",
    "    bigram_freq = sorted(bigram_freq, reverse=True)[:3]\n",
    "    return [t[1] for t in bigram_freq if t[0] > 0 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Função que expande a consulta, recebendo um único termo e realizando uma consulta \"OR\" entre o termo passado e as 3 palavras que mais ocorrem com ele "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expandedSearch(word) :\n",
    "    top3 = top3_bigram(word)\n",
    "    return searchOr(word, *top3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exemplos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escolha livremente três termos de consulta e responda o seguinte:\n",
    "##### Quais os termos retornados para a expansão de cada consulta?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'governo': 'federal', 'dilma' e 'estado'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['federal', 'dilma', 'estado']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top3_bigram('governo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'projeto': 'lei', 'politico' e 'poder'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lei', 'político', 'poder']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top3_bigram('projeto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'mandato': 'dilma', 'presidente' e 'deputado'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dilma', 'presidente', 'deputado']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top3_bigram('mandato')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Você acha que esses termos são de fato relacionados com a consulta original? Justifique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sim, dado o contexto de noticias politicas em questão, as palavras retornadas se mostram muito relacionadas aos termos originais, é comum nesse universo \"governo federal\", \"projeto de lei\"...\n",
    "Alguns são claramente bigramas (\"governo dilma\"), alguns tem uma stopword entre eles (\"governo do estado\").\n",
    "O bigrama que parece mais distante é 'projeto' e 'poder', ainda assim são relacionados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Compare os documentos retornados para a consulta original com a consulta expandida. Quais resultados você acha que melhor capturam a necessidade de informação do usuário? Por que?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como pode-se ver abaixo, a consulta expandida chega a retornar mais de 5x mais documentos que a consulta original, que já é um número bem grande de documentos, isso sem dúvida faz a consulta original melhor, porque normalmente o usuário tenta olhar cerca de 10 resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GOVERNO - Consulta original: 4983 | Consulta expandida: 7142\n"
     ]
    }
   ],
   "source": [
    "print('GOVERNO - Consulta original: {} | Consulta expandida: {}'.format(len(search('governo')), len(expandedSearch('governo'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJETO - Consulta original: 1156 | Consulta expandida: 4773\n"
     ]
    }
   ],
   "source": [
    "print('PROJETO - Consulta original: {} | Consulta expandida: {}'.format(len(search('projeto')), len(expandedSearch('projeto'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MANDATO - Consulta original: 1219 | Consulta expandida: 6442\n"
     ]
    }
   ],
   "source": [
    "print('MANDATO - Consulta original: {} | Consulta expandida: {}'.format(len(search('mandato')), len(expandedSearch('mandato'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 dos documentos retornados apenas na consulta expandida para GOVERNO:\n",
      "[8194, 8195, 8196, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "print('5 dos documentos retornados apenas na consulta expandida para GOVERNO:\\n{}'.format(list(set(expandedSearch('governo'))-set(search('governo')))[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 dos documentos retornados apenas na consulta expandida para PROJETO:\n",
      "[1, 2, 3, 13, 15]\n"
     ]
    }
   ],
   "source": [
    "print('5 dos documentos retornados apenas na consulta expandida para PROJETO:\\n{}'.format(list(set(expandedSearch('projeto'))-set(search('projeto')))[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 dos documentos retornados apenas na consulta expandida para MANDATO:\n",
      "[9, 12, 13, 15, 17]\n"
     ]
    }
   ],
   "source": [
    "print('5 dos documentos retornados apenas na consulta expandida para MANDATO:\\n{}'.format(list(set(expandedSearch('mandato'))-set(search('mandato')))[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na minha opinião, alguns documentos que são retornados apenas pela consulta expandida não são nada adequados a consulta original (a maioria dos que olhei, pelo menos), dada a naturesa dos dados, todos eles estão ligados a política, para 'governo' e 'mandato' não acho que nenhum dos documentos são relevantes, 'projeto' tem os documentos 2 e 3 relevantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### A expansão de consultas é mais adequada para melhorar o recall ou o precision? Por que?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall, uma vez que embora ela vá retornar documentos relevantes que eventualmente não contém o termo original da consulta, ela retornará outros domumentos que contém os termos da espansão que não são relevantes para o usuário, é um grande aumento no número de documentos retornados para retornar mais documentos relevantes, mas muito deles não serão relevantes, diminuindo a precisão e aumentando o recall"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
